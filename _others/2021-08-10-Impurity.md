---
title: Impurity
author: Yu Donghwi
date: 2021-08-10
category: Jekyll
layout: post
---

## Definition ##

Gradient descent는 target이 numerical할 때 사용되는 ML/DL 회귀 모델링의 최적화 메커니즘이다. 반면 불순도(Impurity)는 target이 categorical할 때 사용되는 ML/DL 분류 모델링의 최적화 메커니즘이다. Decision Tree는 각 Node에서 불순도를 최소화하는 분류 기준으로 가지치기를 진행하며 불순도를 측정하는 지표로 엔트로피 지수, 지니 지수, 카이제곱 통계량이 있다. 이때 root node부터 leaft node까지 max_depth에 도달하는 과정에서 선택한 측정 지표를 일관되게 유지해야하며 max_depth에 이르지 않았더라도 불순도가 0이 되는 순간 학습은 종료된다.

<br>

> ##### Information Gain
>
> information gain(정보 획득량) = 1 - impurity
> $ IG = 1 - I $
{: .block-tip }




### Gini Index ###
경제학에서 사용하는 소득불평등도와 동일한 개념으로서 불균형에 대한 측도이다. 지니 지수가 클수록 불순도가 높아지며 측정 방식은 아래와 같다.

$$Gini = 1 - \sum_{i=1}^{n} (p_{i})^2$$

### Entropy Index ###
열역학에서 사용하는 자유도와 동일한 개념으로서 무질서에 대한 측도이다. 엔트로피 지수가 클수록 불순도가 높아지며 측정 방식은 아래와 같다.

$$Entropy = \sum_{i=1}^{n} -p_{i} \cdot (\log_{2}^{p_{i}})$$

### Chi square statistics ###
통계학에서 사용하는 적합성 검정의 통계량과 동일한 개념으로서 기대로부터의 오차에 대한 측도이다. 카이제곱 통계량이 클수록 불순도가 높아지며 측정 방식은 아래와 같다.

(k=범주의 수, O=관측 도수, E=기대 도수)

$$ \chi^2 = \sum_{i=1}^{k} \frac{(O_{i} - E_{i})^2}{E_{i}}$$




## Example ##


### Gini Index ###

> ##### 모형의 가정
>
> 각 노드에서 가지를 두 번만 치는 Binary Tree를 가정(e.g. sklearn)하여 학생의 성적을 분류하는 모델을 설계한다.
> (소수점 둘째자리까지 반올림)
{: .block-warning }

![](https://github.com/user-attachments/assets/3c6b5c0b-7ef8-4ccd-b130-bd99dfe7dd3a)


#### Impurity of root node ####

   $ I_{0} =  1 - (P_{합격})^2 - (P_{불합격})^2 $

   $ = 1 - (\frac{6}{10})^2 - (\frac{4}{10})^2 $

   $= 0.48$ 

#### Impurity of first node ####

##### 분류 기준 : 학습 방식 Left = {그룹 학습}, Right = {개인 학습, 온라인 학습} #####

   $ I = 1 - P_{Left} \cdot (P_{합격 \vert Left})^2 - P_{Left} \cdot (P_{불합격 \vert Left})^2 -P_{Right} \cdot (P_{합격 \vert Right})^2 - P_{Right} \cdot (P_{불합격 \vert Right})^2 $

   $ = 1 - P_{Left}\cdot((P_{합격 \vert Left})^2 + (P_{불합격 \vert Left})^2)- P_{Right}\cdot((P_{합격 \vert Right})^2 + (P_{불합격 \vert Right})^2) $

   $ = 1 - P_{Left} \cdot (1-2 \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left}) - P_{Right} \cdot (1-2 \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right}) $

   $ = 1 - P_{left} - P_{right} + 2 \cdot P_{Left} \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left} + 2 \cdot P_{Right} \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right} $

   $ = 2 \cdot (P_{Left} \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left} + P_{Right} \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right} ) $

   $ = 2 \cdot (0.8 + 0.12) $

   $ = 0.4 $

   $ \Delta IG = 0.48 - 0.4 = 0.08 $

<br>

##### 분류 기준 : 학습 방식 Left = {개인 학습}, Right = {그룹 학습, 온라인 학습} #####

   $ I = 2 \cdot (P_{Left} \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left} + P_{Right} \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right} ) $

   $ = 2 \cdot (\frac{1}{15} + \frac{1}{7}) $

   $ = 0.419 ... $

   $ \Delta IG = 0.48 - 0.42 = 0.06 $

<br>

##### 분류 기준 : 학습 방식 Left = {온라인 학습}, Right = {그룹 학습, 개인 학습} #####

   $ I = 2 \cdot (P_{Left} \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left} + P_{Right} \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right} ) $

   $ = 2 \cdot (\frac{1}{20} + \frac{3}{16}) $

   $ = 0.475 $

   $ \Delta IG = 0.48 - 0.48 = 0 $

<br>

##### 분류 기준 : 과목 Left = {수학}, Right = {영어, 과학} #####

   $ I = 2 \cdot (P_{Left} \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left} + P_{Right} \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right} ) $

   $ = 2 \cdot (\frac{1}{15} + \frac{6}{35}) $

   $ = 0.476... $

   $ \Delta IG = 0.48 - 0.48 = 0 $

<br>

##### 분류 기준 : 과목 Left = {영어}, Right = {수학, 과학} #####

   $ I = 2 \cdot (P_{Left} \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left} + P_{Right} \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right} ) $

   $ = 2 \cdot (0 + \frac{3}{35}) $

   $ = 0.171... $

   $ \Delta IG = 0.48 - 0.17 = 0.31 $

<br>

##### 분류 기준 : 과목 Left = {과학}, Right = {영어, 수학} #####

   $ I = 2 \cdot (P_{Left} \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left} + P_{Right} \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right} ) $

   $ = 2 \cdot (0 + \frac{2}{15}) $

   $ = 0.266... $

   $ \Delta IG = 0.48 - 0.27 = 0.21 $

<br>

#### 소결 ####

![](https://github.com/user-attachments/assets/15dd0cdc-cc56-4e6d-a4a3-8bd44bd9d7e5)


First node : Left = {영어}, Right = {수학, 과학}, $\Delta IG_{1} = 0.31 $

불순도가 가장 적어 가장 많은 정보를 획득하는 분류 기준에 따라 영어 학습 여부에 따라 학생들을 분류하였다.

그 결과 첫 단계에서 영어를 수강한 3명의 학생이 '불합격'으로 분류 되어 Left side from first node의 학습은 종료된다.

#### Impurity of second node #####

##### Right side data from first node #####

![](https://github.com/user-attachments/assets/6c3b7626-c0a7-4b8b-b9ff-bd377e727502)

##### 분류 기준 : 학습 방식 Left = {그룹 학습}, Right = {개인 학습, 온라인 학습} #####

   $ I = 2 \cdot (P_{Left} \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left} + P_{Right} \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right} ) $

   $ = 2 \cdot (0 + \frac{2}{21}) $

   $ = 0.190 ... $

   $ \Delta IG = 0.17 - 0.19 = -0.02... $

<br>

##### 분류 기준 : 학습 방식 Left = {온라인 학습}, Right = {개인 학습, 그룹 학습} #####

   $ I = 2 \cdot (P_{Left} \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left} + P_{Right} \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right} ) $

   $ = 2 \cdot (0 + \frac{5}{42}) $

   $ = 0.238 ... $
   
   $ \Delta IG = 0.17 - 0.24 = -0.07... $

<br>

##### 분류 기준 : 학습 방식 Left = {온라인 학습}, Right = {개인 학습, 그룹 학습} #####

   $ I = 2 \cdot (P_{Left} \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left} + P_{Right} \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right} ) $

   $ = 2 \cdot (\frac{1}{14} + 0) $

   $ = 0.142... $
   
   $ \Delta IG = 0.17 - 0.14 = 0.03... $

<br>

##### 분류 기준 : 과목 Left = {수학}, Right = {과학} #####

   $ I = 2 \cdot (P_{Left} \cdot P_{합격 \vert Left} \cdot P_{불합격 \vert Left} + P_{Right} \cdot P_{합격 \vert Right} \cdot P_{불합격 \vert Right} ) $

   $ = 2 \cdot (\frac{2}{21} + 0) $

   $ = 0.190... $
   
   $ \Delta IG = 0.17 - 0.19 = -0.02... $

<br>

#### 소결 ####

![](https://github.com/user-attachments/assets/c0a9b38d-70e2-48cb-8f2d-985719bae2cc)


Second node : Left = {개인 학습}, Right = {온라인 학습, 그룹 학습}, $ \Delta IG_{2} = 0.03 $

불순도가 가장 적어 가장 많은 정보를 획득하는 분류 기준에 따라 개인 학습을 하는지 여부 따라 학생들을 분류하였다.

그 결과 두 번째 단계에서 개인 학습을 하는 5명의 학생이 '합격'으로 분류 되어 Right side from second node의 학습은 종료된다.

#### Impurity of third node #####

##### Left side data from first node #####

![](https://github.com/user-attachments/assets/a369f1d9-5751-48a6-a3d6-df0ea4a2fd42)

#### Impurity of third node #####

![](https://github.com/user-attachments/assets/91d16738-e6ae-4c4b-8312-97e8446a2150)


불순도가 0이 되어 학습이 종료되어 위와 같은 Decision tree 모형이 완성되었다.
이후 모델은 새로운 데이터에 대해서도 이와 같은 절차를 걸쳐 분류 결과 예측치를 제공한다.


#### python ####

python을 통해 scikitlearn DecisionTreeClassifier ML 모델 실행시 동일한 결과가 출력됨을 알 수 있다. 

>##### 유의사항
>
> 이때 criterion의 defalut가 gini이므로 다른 impurity index를 사용할 때는 criterion에 유의해야한다.
{: .block-danger}

```yaml
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
df = pd.read_csv('/content/raw data.csv', encoding='cp949')

from sklearn.preprocessing import LabelEncoder
le =  LabelEncoder()
for col in df.columns :
  le.fit(df[col])
  df[col] = le.transform(df[col])

x = df[['학습방식', '과목']]
y = df['성적']

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=123, criterion = "gini")
model = dt.fit(x,y)

from sklearn import tree
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(20,10))
fig = tree.plot_tree(model, feature_names = ['학습방식', '과목'], class_names=['0', '1'], filled=True)
```
![](https://github.com/user-attachments/assets/05f6af55-ba47-439d-a69d-0c2dbb6ff869)


### Entropy Index ###

> ##### 모형의 가정
>
> 각 노드에서 3개 이상의 가지를 칠 수 있는 Multi Branch Tree를 가정하여 학생의 성적을 분류하는 모델을 설계한다.
> (소수점 둘째자리까지 반올림)
{: .block-warning }

![](https://github.com/user-attachments/assets/3c6b5c0b-7ef8-4ccd-b130-bd99dfe7dd3a)


#### Impurity of root node ####

   $ I_{0} = -P_{합격} \cdot \log_{2}^{P_{합격}} - P_{불합격} \cdot \log_{2}^{P_불합격} $

   $ = - 0.6 \cdot (-0.74) - 0.4 \cdot (-1.32) $

   $ = 0.97 $ 

#### Impurity of first node #####

##### 분류 기준 : 학습 방식 #####

   $ I = -(P_{합격 \vert 그룹 학습 } \cdot \log_{2}^{P_{합격 \vert 그룹 학습}})\cdot P_{그룹 학습} -(P_{불합격 \vert 그룹 학습 } \cdot \log_{2}^{P_{불합격 \vert 그룹 학습}})\cdot P_{그룹 학습} $ 
   
   $ -(P_{합격 \vert 개인 학습 } \cdot \log_{2}^{P_{합격 \vert 개인 학습}})\cdot P_{개인 학습} -(P_{불합격 \vert 개인 학습 } \cdot \log_{2}^{P_{불합격 \vert 개인 학습}})\cdot P_{개인 학습} $
   
   $ -(P_{합격 \vert 온라인 학습 } \cdot \log_{2}^{P_{합격 \vert 온라인 학습}})\cdot P_{온라인 학습} -(P_{불합격 \vert 온라인 학습 } \cdot \log_{2}^{P_{불합격 \vert 온라인 학습}})\cdot P_{온라인 학습} $

   $ = - 0.5 \cdot (-0.26) - 0.5 \cdot (-0.46) - 0.3 \cdot (-0.53) - 0.3 \cdot (-0.39) - 0.2 \cdot (-0.5) - 0.2 \cdot (-0.5) $

   $ = 0.84 $

   $ \Delta IG = 0.97 - 0.84 = 0.13 $

<br>

##### 분류 기준 : 과목 #####

   $ I = -(P_{합격 \vert 수학 } \cdot \log_{2}^{P_{합격 \vert 수학}})\cdot P_{수학} -(P_{불합격 \vert 수학 } \cdot \log_{2}^{P_{불합격 \vert 수학}})\cdot P_{수학} $ 
   
   $ -(P_{합격 \vert 영어 } \cdot \log_{2}^{P_{합격 \vert 영어}})\cdot P_{영어} -(P_{불합격 \vert 영어 } \cdot \log_{2}^{P_{불합격 \vert 영어}})\cdot P_{영어} $
   
   $ -(P_{합격 \vert 과학 } \cdot \log_{2}^{P_{합격 \vert 과학}})\cdot P_{과학} -(P_{불합격 \vert 과학 } \cdot \log_{2}^{P_{불합격 \vert 과학}})\cdot P_{과학} $

   $ = - 0.3 \cdot (-0.39) - 0.3 \cdot (-0.53) - 0.3 \cdot (0) - 0.3 \cdot (0) - 0.4 \cdot (0) - 0.4 \cdot (0) $

   $ = 0.28 $

   $ \Delta IG = 0.97 - 0.28 = 0.69 $

<br>

#### 소결 ####

![](https://github.com/user-attachments/assets/d6092e2c-35be-444e-bb6c-4c9e431fb2a9)


first node : {수학, 영어, 과학}, $\Delta IG_{1} = 0.69 $

불순도가 가장 적어 가장 많은 정보를 획득하는 분류 기준에 따라 학습 과목을 기준으로 학생을 분류하였다.

그 결과 영어를 공부한 학생은 모두 불합격으로, 과학을 공부한 학생들은 모두 합격으로 분류 되었다.

또한 다진분류모델이므로 second node는 당연히 학습 방식에 의해 구분되며 불순도가 0이 되고 학습이 종료될 것이다.


### Chi square statistics ###

>##### 모형의 가정
>
> 각 노드에서 3개 이상의 가지를 칠 수 있는 Multi Branch Tree를 가정하여 학생의 성적을 분류하는 모델을 설계한다.
> (소수점 첫째자리에서 반올림)
{: .block-warning }

![](https://github.com/user-attachments/assets/3c6b5c0b-7ef8-4ccd-b130-bd99dfe7dd3a)

##### Cross Table

![](https://github.com/user-attachments/assets/b5621627-2314-43e2-80fa-3dbb532ada06)

<br>

$ I_{학습 방법} = \frac{(4-3)^2}{3} + \frac{(1-2)^2}{2} + \frac{(1-1)^2}{1} + \frac{(1-2)^2}{4} + \frac{(2-1)^2}{1} + \frac{(1-1)^2}{1} = 2 $

$ I_{과목} = \frac{(2-2)^2}{2} + \frac{(0-1)^2}{2} + \frac{(4-0)^2}{2} + \frac{(1-1)^2}{1} + \frac{(3-1)^2}{1} + \frac{(0-1)^2}{2} = 10 $

<br>
카이제곱통계량이 더 낮은 '학습 방법'을 첫 노드의 분류 기준으로 채택하며 이는 '과목'을 첫 노드의 분류 기준으로 선택한 엔트로피 지수와 상반된다.


<br>
<br>
<br>
<br>